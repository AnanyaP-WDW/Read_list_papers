# Reading list

**Interests**
Deep learning, Optimization, Neuroscience and Biomedical engineering; Ideally at the intersection of two or more of the above mentioned themes. 

**Motivation**:
Andrew Ng's [CS:230 Deep Learning| Autumn 2018 - Reading Research and Career Advice.](https://www.youtube.com/watch?v=733m6qBH-jI)

**Objectives**
> Learn the most; try to do important meaningful work

**Learnings from the video**:

**Learning Philosophy**
> Steady learning; not short bursts + Spaced Repetetion

**Learning Approaches**

1) Reading research:
- Compile a list of papers, medium and github posts - Stops indecision and provides a clear roadmap.
- According to interest and importance, skip around a bunch of papers.

2) How to read papers? - Take multiple passes
- Step 1: Title + Abstract + Figures
- Step 2: Introduction + Conclusions + Figures + Related work (based on interest) + Skim rest
- Step 3: Whole paper - Math
- Step 4: Whole paper - Parts that don't make sense

3) How to know you have understood a paper? (*first and second points are vital*)
- What the authors tried to accomplish?
- What were the key elements of the approach?
- What can you use for yourself?
- What other references do you want to follow?

4) Sources of Papers:
- ML subreddit
- Twitter 
- NIPS | ICML | ICLR proceedings

5) Understanding Math:
- Read the paper accroding to point 3).
- Try to rederive the math from scratch by keeping the paper away.

6) Understanding Code:
- For light weight understanding - Run the open source code
- For deeper understanding - Reimplement from scratch

## Papers
### Theme - General DL
- [x] [Understanding Deep Learining Requires Rethinking Generalization](https://arxiv.org/pdf/1611.03530.pdf)
- [x] [A Closer Look at Memorization in Deep Networks](https://arxiv.org/pdf/1706.05394.pdf)
- [ ] [Deep Nets don't Learn via Memorization](https://www.researchgate.net/profile/Asja_Fischer/publication/315799530_Deep_Nets_Don%27t_Learn_via_Memorization/links/58e63a73a6fdcc6800b44d11/Deep-Nets-Dont-Learn-via-Memorization.pdf)
- [ ] [When Does Label Smoothing Help?](https://arxiv.org/pdf/1906.02629.pdf)


### Theme - Optimization
- [ ] [Three Factors Influencing Minima in SGD](https://arxiv.org/pdf/1711.04623.pdf)
- [ ] [On the relation between the sharpest directions of DNN loss and the SGD step length](https://arxiv.org/pdf/1807.05031.pdf)
- [ ] [ADAM: A Method for Stochastic Optimization](https://arxiv.org/pdf/1412.6980.pdf)

### Theme - NLP
- [ ] [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
- [x] [Simple Unsupervised Keyphrase Extraction using Sentence Embeddings](https://arxiv.org/pdf/1801.04470.pdf)
- [x] [On The Use of Arxiv as a Dataset](https://arxiv.org/pdf/1905.00075.pdf)
- [ ] [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)
- [ ] [How does BERT capture semantics? A closer look at polysemous words](https://www.aclweb.org/anthology/2020.blackboxnlp-1.15.pdf)

### Theme - NLP fundamental tutorials
- [ ] [Porter stemmer - Stemming](http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf)
- [ ] [fastText: Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)

### Theme - Bio NLP
- [ ] [HunFlair: An Easy-to-Use Tool for State-of-the-Art Biomedical Named Entity Recognition](https://arxiv.org/pdf/2008.07347.pdf)
- [ ] [Segmenting DNA sequence into ‘words’](https://arxiv.org/pdf/1202.2518.pdf)
- [ ] [DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1.full)

### Theme - Medical Diagnostics
- [ ] [Epidermal electronics for noninvasive, wireless,quantitative assessment of ventricular shuntfunction in patients with hydrocephalus](http://rogersgroup.northwestern.edu/files/2018/shuntsstm.pdf)

### Theme - Unsupervised Learning
- [ ] [SCAN: Learning to Classify Images without Labels](https://arxiv.org/abs/2005.12320)
- [ ] [hdbscan : Rates of convergence for the cluster tree](https://cseweb.ucsd.edu/~dasgupta/papers/tree.pdf)
